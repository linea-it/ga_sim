{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## ga_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jn is intended to create simulations of dwarf galaxies and globular clusters using as field stars the catalog of DES. These simulations will be later copied to gawa jn, a pipeline to detect stellar systems with field's stars. In principle this pipeline read a table in data base with g and r magnitudes, subtract the extinction in each band, and randomize the positions in RA and DEC in order to avoid stellar systems in the FoV. The star clusters are inserted later, centered in each HP pixel with specific nside.\n",
    "\n",
    "To complete all the steps you just have to run all the cells below in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, install the packages not available in the image via terminal. Restart the kernel and so you can run the cell bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "import healpy as hp\n",
    "import astropy.io.fits as fits\n",
    "from astropy.table import Table\n",
    "from astropy.io.fits import getdata\n",
    "import sqlalchemy\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import parsl\n",
    "from parsl.app.app import python_app, bash_app\n",
    "from parsl.configs.local_threads import config\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from ga_sim import (\n",
    "    get_av,\n",
    "    make_footprint,\n",
    "    faker,\n",
    "    join_cat,\n",
    "    write_sim_clus_features,\n",
    "    download_iso,\n",
    "    read_cat,\n",
    "    gen_clus_file,\n",
    "    read_error,\n",
    "    clus_file_results,\n",
    "    join_cats_clean,\n",
    "    split_files,\n",
    "    clean_input_cat\n",
    ")\n",
    "\n",
    "parsl.clear()\n",
    "parsl.load(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the items of the configuration for field stars and simulations. A small description follows as a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main settings:\n",
    "nside_ini = 64  # the nside in which pixel star clusters will be inserted in the center\n",
    "nside_ftp = 4096  # nside of the footprint map\n",
    "nside3 = 2 ** 18  # pixelization of the star's position\n",
    "ra_min, ra_max = 45, 55  # 5., 85. # limits in ra of the simulated map\n",
    "dec_min, dec_max = -30, -20  # -60., -20. # limits in dec of the simulated map\n",
    "mmin, mmax, cmin, cmax = 17.00, 24.00, - \\\n",
    "    0.4, 1.6  # magnitude and color limits of stars\n",
    "ell_min, ell_max = 0.00, 0.20  # limits in ellipticity of star clusters simulated\n",
    "pa_min, pa_max = 0., 180.  # limits in position angle\n",
    "# limits for the visible mass of the clusters,\n",
    "log10_mass_min, log10_mass_max = 3.00, 3.10\n",
    "age_simulation = 1.e10  # in years\n",
    "Z_simulation = 0.001  # Assuming Z_sun = 0.0152\n",
    "av_simulation = 0.000  # It must be zero.\n",
    "padova_version_code = '3.6'\n",
    "\n",
    "# avoiding simulate many stars unseen in the catalog given the limiting magnitude\n",
    "mM_min, mM_max = 20.001, 20.002  # limits in modulus distance\n",
    "log10_rexp_min, log10_rexp_max = 0.60, 1.00  # limits in exponential radius\n",
    "\n",
    "survey = 'des'  # survey name\n",
    "# fraction of binaries (amount of stars in binaries = Nbinaries / Ntotal)\n",
    "frac_bin = 0.5\n",
    "IMF_author = 'Kroupa'  # selection of initial mass function\n",
    "border_extract = 1.  # remove star clusters in the edge of the map, in degrees\n",
    "\n",
    "# Diretório para os resultados\n",
    "results_path = Path(\"results\")\n",
    "results_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reading reddening files\n",
    "hdu_ngp = fits.open(\"sample_data/SFD_dust_4096_ngp.fits\", memmap=True)\n",
    "ngp = hdu_ngp[0].data\n",
    "\n",
    "hdu_sgp = fits.open(\"sample_data/SFD_dust_4096_sgp.fits\", memmap=True)\n",
    "sgp = hdu_sgp[0].data\n",
    "\n",
    "# file to be downloaded with the isochrone info\n",
    "file_iso = 'sample_data/iso_input_down.dat'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the isochrone table with the last improvements from Padova.\n",
    "Printing age and metalicity of isochrone downloaded. Try one more time in case of problems. Sometimes there is a problem with the connection to Padova."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_iso(padova_version_code, survey, Z_simulation,\n",
    "             age_simulation, av_simulation, file_iso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking age and metalicity of the isochrone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_info = np.loadtxt(file_iso, usecols=(1, 2), unpack=True)\n",
    "FeH_iso = iso_info[0][0]\n",
    "logAge_iso = iso_info[1][0]\n",
    "print('[Fe/H]={:.2f}, Age={:.2f} Gyr'.format(FeH_iso, 10**(logAge_iso-9)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpx_ftp = make_footprint(ra_min, ra_max, dec_min, dec_max,\n",
    "                         nside_ftp, output_path=results_path)\n",
    "print(len(hpx_ftp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the catalog and writing as a fits file (to avoid read from the DB many times in the case the same catalog will be used multiple times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from DB and writting as corrected by extinction stars\n",
    "RA, DEC, MAG_G, MAGERR_G, MAG_R, MAGERR_R = read_cat(\n",
    "    'vac_ga_y6.catalog_6048', ra_min, ra_max, dec_min, dec_max, mmin, mmax, cmin, cmax,\n",
    "    \"DES_Y6_Gold_v1_derred.fits\", 1.19863, 0.83734, ngp, sgp, results_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below reads the position, calculates the extinction using the previous function and <br>\n",
    "correct the aparent magnitude (top of the Galaxy), filter the stars for magnitude and color ranges, <br> \n",
    "and writes a file with the original position of the stars and corrected magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation of dwarf galaxies and globular clusters\n",
    "\n",
    "In fact, the dwarf galaxies and globular clusters are very similar in terms of stellar populations. Dwarf galaxies\n",
    "have a half-light radius larger than globular clusters (given the amount of dark matter) with the same absolute magnitude. The code below simulates stars using a Kroupa or Salpeter IMF, and an exponential radius for the 2D distribution of stars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the properties of clusters based on properties stated above. Writting to file 'objects.dat'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RA_pix, DEC_pix, r_exp, ell, pa, dist, mass, mM, hp_sample_un = gen_clus_file(ra_min, ra_max, dec_min, dec_max, nside_ini, border_extract,\n",
    "                                                                              mM_min, mM_max, log10_rexp_min, log10_rexp_max, log10_mass_min,\n",
    "                                                                              log10_mass_max, ell_min, ell_max, pa_min, pa_max, results_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dist stars\n",
    "Reading data from magnitude and errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag1_, err1_, err2_ = read_error('sample_data/errors_Y6.dat', 0.015, 0.015)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now simulating the clusters using 'faker' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@python_app\n",
    "def faker_app(N_stars_cmd, frac_bin, IMF_author, x0, y0, rexp, ell_, pa, dist, hpx, output_path):\n",
    "\n",
    "    faker(\n",
    "        N_stars_cmd,\n",
    "        frac_bin,\n",
    "        IMF_author,\n",
    "        x0,\n",
    "        y0,\n",
    "        rexp,\n",
    "        ell_,\n",
    "        pa,\n",
    "        dist,\n",
    "        hpx,\n",
    "        cmin,\n",
    "        cmax,\n",
    "        mmin,\n",
    "        mmax,\n",
    "        mag1_,\n",
    "        err1_,\n",
    "        err2_,\n",
    "        file_iso,\n",
    "        output_path\n",
    "    )\n",
    "\n",
    "\n",
    "# Diretório dos arquivo _clus.dat gerados pela faker.\n",
    "fake_clus_path = Path(results_path, \"fake_clus\")\n",
    "\n",
    "futures = list()\n",
    "\n",
    "# Cria uma Progressbar (Opcional)\n",
    "with tqdm(total=len(hp_sample_un), file=sys.stdout) as pbar:\n",
    "    pbar.set_description(\"Submit Parsls Tasks\")\n",
    "\n",
    "    # Submissão dos Jobs Parsl\n",
    "    for i in range(len(hp_sample_un)):\n",
    "        # TODO: Descrever essa operação, esse valor 0.54 é um parametro? é fixo para sempre?\n",
    "        # Deveria estar na lista de variaveis no inicio do JN?\n",
    "        N_stars_cmd = int(mass[i] / 0.54)\n",
    "        # os.register_at_fork(after_in_child=lambda: _get_font.cache_clear())\n",
    "        futures.append(\n",
    "            faker_app(\n",
    "                N_stars_cmd,\n",
    "                frac_bin,\n",
    "                \"Kroupa\",\n",
    "                RA_pix[i],\n",
    "                DEC_pix[i],\n",
    "                r_exp[i],\n",
    "                ell[i],\n",
    "                pa[i],\n",
    "                dist[i],\n",
    "                hp_sample_un[i],\n",
    "                output_path=fake_clus_path\n",
    "            )\n",
    "        )\n",
    "\n",
    "        pbar.update()\n",
    "# Fim da submissão dos Jobs,\n",
    "\n",
    "\n",
    "# Espera todas as tasks Parsl terminarem\n",
    "# Este loop fica monitarando as parsl.futures\n",
    "# Até que todas tenham status done.\n",
    "# Esse bloco todo é opicional\n",
    "\n",
    "# Progressbar para acompanhar as parsl.tasks.\n",
    "print(\"Tasks Done:\")\n",
    "with tqdm(total=len(futures), file=sys.stdout) as pbar2:\n",
    "    # is_done é um array contendo True ou False para cada task\n",
    "    # is_done.count(True) retorna a quantidade de tasks que já terminaram.\n",
    "    is_done = list()\n",
    "    done_count = 0\n",
    "    while is_done.count(True) != len(futures):\n",
    "        is_done = list()\n",
    "        for f in futures:\n",
    "            is_done.append(f.done())\n",
    "\n",
    "        # Só atualiza a pbar se o valor for diferente.\n",
    "        if is_done.count(True) != done_count:\n",
    "            done_count = is_done.count(True)\n",
    "            # Reset é necessário por que a quantidade de iterações\n",
    "            # é maior que a quantidade de jobs.\n",
    "            pbar2.reset(total=len(futures))\n",
    "            # Atualiza a pbar\n",
    "            pbar2.update(done_count)\n",
    "\n",
    "        if done_count < len(futures):\n",
    "            sleep(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now functions to join catalogs of simulated clusters and field stars, and to estimate signal-to-noise ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le os arquivos _clus.dat do diretório \"result/fake_clus\"\n",
    "# Gera o arquivo \"result/<survey>_mockcat_for_detection.fits\"\n",
    "mockcat = join_cat(\n",
    "    ra_min,\n",
    "    ra_max,\n",
    "    dec_min,\n",
    "    dec_max,\n",
    "    hp_sample_un,\n",
    "    survey,\n",
    "    RA,\n",
    "    DEC,\n",
    "    MAG_G,\n",
    "    MAG_R,\n",
    "    MAGERR_G,\n",
    "    MAGERR_R,\n",
    "    nside_ini,\n",
    "    mmax,\n",
    "    mmin,\n",
    "    cmin,\n",
    "    cmax,\n",
    "    input_path=fake_clus_path,\n",
    "    output_path=results_path)\n",
    "print(mockcat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, split the catalog with simulated clusters into many files according HP schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpx_cats_path = Path(results_path, \"hpx_cats\")\n",
    "list_files_HPX = split_files(mockcat, 'ra', 'dec', nside_ini, str(hpx_cats_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_clus_feat = write_sim_clus_features(mockcat, hp_sample_un, nside_ini, mM, output_path=results_path)\n",
    "print(sim_clus_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge both files in a single file using join command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system('join --nocheck-order results/n_stars.dat results/objects.dat > results/star_clusters_simulated.dat')\n",
    "clus_file_results(results_path, \"star_clusters_simulated.dat\", sim_clus_feat, 'results/objects.dat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "A few plots to characterize the simulated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ga_sim.plot import general_plots\n",
    "from ga_sim.plot import plot_ftp\n",
    "from ga_sim.plot import plots_ang_size\n",
    "from ga_sim.plot import plots_ref\n",
    "from ga_sim.plot import plot_err\n",
    "from ga_sim.plot import plot_clusters_clean\n",
    "\n",
    "star_clusters_simulated = Path(\"results\", \"star_clusters_simulated.dat\")\n",
    "general_plots(star_clusters_simulated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot footprint map to check area.\n",
    "hpx_ftp = Path(\"results\", \"ftp_4096_nest.fits\")\n",
    "\n",
    "plot_ftp(hpx_ftp, star_clusters_simulated, mockcat, ra_max, ra_min, dec_min, dec_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots to analyze the simulated clusters.\n",
    "ftp_fits = Path(\"data\", \"ftp_4096_nest.fits\")\n",
    "\n",
    "# Diretório onde estão os arquivo _clus.dat\n",
    "clus_path = Path(\"results\")\n",
    "output_plots = Path(\"results/plots\")\n",
    "plots_ang_size(star_clusters_simulated, clus_path, mmin, mmax, cmin, cmax, output_plots)\n",
    "\n",
    "# TODO: Seria interessante dividir essa função em duas\n",
    "# Uma para os plots de _clus.dat\n",
    "# Outra para os 4 plots do final.\n",
    "# Dessa forma a geração dos plots _clus.dat poderia ser paralelizada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots são gerados no diretório seria interessante aparecer aqui no JN\n",
    "plots_ref(FeH_iso, star_clusters_simulated, output_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying the main files to the folder where the Gawa code will be able to detect the simulated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots to analyze the simulated clusters.\n",
    "mockcat = Path(\"results\", \"des_mockcat_for_detection.fits\")\n",
    "\n",
    "plot_err(mockcat, output_plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Close Stars\n",
    "results_path = Path(\"results\")\n",
    "final_cat = Path(results_path, \"cat_clean.fits\")\n",
    "ra_str, dec_str = 'ra', 'dec'\n",
    "path_split = 'results/hpx_cats/'\n",
    "\n",
    "ipix_cats = split_files(mockcat, ra_str, dec_str, nside_ini, path_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@python_app\n",
    "def clean_input_cat_app(file_name, ra_str, dec_str, nside):\n",
    "\n",
    "    clean_input_cat(\n",
    "                    file_name,\n",
    "                    ra_str,\n",
    "                    dec_str,\n",
    "                    nside\n",
    "                    )\n",
    "\n",
    "futures = list()\n",
    "\n",
    "# Cria uma Progressbar (Opcional)\n",
    "with tqdm(total=len(ipix_cats), file=sys.stdout) as pbar:\n",
    "    pbar.set_description(\"Submit Parsls Tasks\")\n",
    "\n",
    "    # Submissão dos Jobs Parsl\n",
    "    for i in ipix_cats:\n",
    "        futures.append(\n",
    "            clean_input_cat_app(i, ra_str, dec_str, 2 ** 17)\n",
    "        )\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "# Espera todas as tasks Parsl terminarem\n",
    "# Este loop fica monitarando as parsl.futures\n",
    "# Até que todas tenham status done.\n",
    "# Esse bloco todo é opcional\n",
    "\n",
    "print(\"Tasks Done:\")\n",
    "with tqdm(total=len(futures), file=sys.stdout) as pbar2:\n",
    "    # is_done é um array contendo True ou False para cada task\n",
    "    # is_done.count(True) retorna a quantidade de tasks que já terminaram.\n",
    "    is_done = list()\n",
    "    done_count = 0\n",
    "    while is_done.count(True) != len(futures):\n",
    "        is_done = list()\n",
    "        for f in futures:\n",
    "            is_done.append(f.done())\n",
    "\n",
    "        # Só atualiza a pbar se o valor for diferente.\n",
    "        if is_done.count(True) != done_count:\n",
    "            done_count = is_done.count(True)\n",
    "            # Reset é necessário por que a quantidade de iterações\n",
    "            # é maior que a quantidade de jobs.\n",
    "            pbar2.reset(total=len(futures))\n",
    "            # Atualiza a pbar\n",
    "            pbar2.update(done_count)\n",
    "\n",
    "        if done_count < len(futures):\n",
    "            sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipix_clean_cats = [i.split('.')[0] + '_clean.fits' for i in ipix_cats]\n",
    "join_cats_clean(ipix_clean_cats, final_cat, ra_str, dec_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_clean(ipix_cats, ipix_clean_cats, nside_ini, ra_str, dec_str, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ga_sim",
   "language": "python",
   "name": "ga_sim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
